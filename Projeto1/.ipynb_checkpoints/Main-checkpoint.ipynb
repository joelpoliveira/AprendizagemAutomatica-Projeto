{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d15fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"parkinsons_updrs.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17208b1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6dc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate X from y\n",
    "X = data.drop(columns=[\"total_UPDRS\", \"motor_UPDRS\"])\n",
    "y = data[\"motor_UPDRS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate Training set from Independent Validations Set\n",
    "(X_train, X_IND, \n",
    " y_train, y_IND) = train_test_split(X, y, test_size=361, random_state=361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e63906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join X and y from the training set\n",
    "data_train = pd.concat((X_train, y_train), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ee2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[data_train[\"subject#\"]==20].plot(x=\"test_time\", y = \"motor_UPDRS\", ls=\"\", marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a7827",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.sex.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837ac27",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=data_train, x=\"sex\", y=\"motor_UPDRS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=[\"subject#\", \"test_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_statistics_simple(model, X, y):    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=21)\n",
    "\n",
    "    #fit model\n",
    "    temp_model = model.fit(X_train, y_train)\n",
    "    \n",
    "    #get predictions on train set \n",
    "    train_preds = temp_model.predict(X_train)\n",
    "    #calculate rmse on training set\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "    \n",
    "    #get predictions on test set\n",
    "    preds = temp_model.predict(X_test)\n",
    "    #calculate rmse on test set\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    \n",
    "    #get statistics\n",
    "    pearse = ( np.corrcoef(y_test, preds)[0,1] )\n",
    "    rve = ( explained_variance_score(y_test, preds) )\n",
    "    max_err = ( np.abs(y_test - preds).max() )\n",
    "    \n",
    "    #return statistics estimations\n",
    "    return (\n",
    "        train_rmse,\n",
    "        test_rmse,\n",
    "        pearse,\n",
    "        rve,\n",
    "        max_err\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5cc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_statistics_cv(model, X, y):\n",
    "    kf = KFold(n_splits=16)\n",
    "    \n",
    "    #statistic arrays\n",
    "    pearse = []\n",
    "    test_rmse = []\n",
    "    train_rmse = []\n",
    "    rve = []\n",
    "    max_err = []\n",
    "    \n",
    "    #Get the train/test folds\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        #Create train/test sets\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        #fit model\n",
    "        temp_model = model.fit(X_train, y_train)\n",
    "        \n",
    "        #get predictions on train set\n",
    "        train_preds = temp_model.predict(X_train)\n",
    "        \n",
    "        #get predictions on test set\n",
    "        preds = temp_model.predict(X_test)\n",
    "        \n",
    "        #get statistics\n",
    "        train_rmse.append(np.sqrt(mean_squared_error(y_train, train_preds)))\n",
    "        \n",
    "        pearse.append( np.corrcoef(y_test, preds)[0,1] )\n",
    "        test_rmse.append( np.sqrt(mean_squared_error(y_test, preds)) )\n",
    "        rve.append( explained_variance_score(y_test, preds) )\n",
    "        max_err.append( np.abs(y_test - preds).max() )\n",
    "    \n",
    "    #return statistics estimations\n",
    "    return (\n",
    "         np.mean(pearse),\n",
    "         np.mean(test_rmse),\n",
    "         np.mean(rve),\n",
    "         np.max(max_err),\n",
    "        np.mean(train_rmse)\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd661c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get_model_statistics(Ridge(), X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ebc31f",
   "metadata": {},
   "source": [
    "# Ridge Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.01, 5, 100)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2391f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns = [\"Pearson\", \"Test RMSE\", \"RVE\", \"Max_err\", \"Train RMSE\"])\n",
    "for alpha in alphas:\n",
    "    (train_rmse, test_rmse, \n",
    "     pearse, rve, max_err) = get_model_statistics_simple(Ridge(alpha=alpha), X_train, y_train)\n",
    "    \n",
    "    results.loc[alpha] = {\n",
    "        \"Pearson\": pearse,\n",
    "        \"Test RMSE\": test_rmse,\n",
    "        \"RVE\":rve,\n",
    "        \"Max_err\": max_err,\n",
    "        \"Train RMSE\": train_rmse\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_df(df, column, ax, title=\"\", x_label=\"\", y_label=\"\", legend=\"\"):\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlabel(x_label)\n",
    "    \n",
    "    ax.plot(df.index, df[column], \"-*\", label=legend)\n",
    "    ax.legend()\n",
    "\n",
    "f = plt.figure()\n",
    "ax = plt.axes()\n",
    "plot_df(results, \"Train RMSE\", ax, legend=\"Train RMSE\")\n",
    "plot_df(results, \"Test RMSE\", ax, legend=\"Test RMSE\", title=\"RMSE\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8cc737",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2, figsize=(12,4))\n",
    "ax[0].plot(results.index, results.Max_err, \"-*\")\n",
    "ax[0].set_title(\"Max Error in the Model\")\n",
    "\n",
    "ax[1].plot(results.index, results.Pearson, '-*')\n",
    "ax[1].set_title(\"Pearson Correlation Coef.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac14b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(columns=X_train.columns)\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    coefs.loc[alpha] = dict(zip(model.feature_names_in_, model.coef_))\n",
    "coefs.plot(figsize=(12,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f514ab52",
   "metadata": {},
   "source": [
    "It seems that the plot alpha/error does not evolves to a \"sweet spot\".\n",
    "Instead of a U like shape, the error evolves in a logarithmic manner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec66e9f7",
   "metadata": {},
   "source": [
    "# Lasso Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIMPLE CROSS VALIDATION\n",
    "coefs=[]\n",
    "rmse_train = []\n",
    "rmse_test = []\n",
    "alphas = np.linspace(0.01, 5, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=21)\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, max_iter=100000).fit(X_train, y_train)\n",
    "    preds_tr=lasso.predict(X_train)\n",
    "    preds_te=lasso.predict(X_test)\n",
    "    rmse_train.append(mean_squared_error(y_train, preds_tr, squared=False))\n",
    "    rmse_test.append(mean_squared_error(y_test, preds_te, squared=False))\n",
    "    coefs.append(lasso.coef_)\n",
    "coefs = np.array(coefs)\n",
    "    \n",
    "plt.plot(alphas, rmse_train, label=\"rmse Train\")    \n",
    "plt.plot(alphas, rmse_test, label=\"rmse Test\")  \n",
    "plt.xlabel(\"Alpha Value\", fontsize=12)\n",
    "plt.ylabel(\"RMSE\", fontsize=12)\n",
    "#plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig(\"lasso_rmse.pdf\", dpi=25, format=\"pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dad3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N,M=X_train.shape\n",
    "plt.figure(figsize=(12,8))\n",
    "for i in range(M):\n",
    "    plt.plot(alphas, coefs[:,i], label=\"Var %d\" % (i+1))\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207efd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas[1:] - alphas[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ef2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "get_model_statistics_cv(model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a329fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_statistics_cv(Ridge(alpha=0), X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc79f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
