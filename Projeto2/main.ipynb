{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea841de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler\n",
    "from sklearn.metrics import (mean_squared_error, max_error, \n",
    "                             confusion_matrix, ConfusionMatrixDisplay, \n",
    "                             classification_report, make_scorer, matthews_corrcoef)\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3232268d-f3e0-47a6-89d2-5c14a2337928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"biodegradable_a.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129e7f99-5a34-4e39-89e7-2810f207dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"Biodegradable\")\n",
    "y = df.Biodegradable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb2da6-1fe4-459e-b983-74091961c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new biodegradable (new_x):  1 if RB else -1\n",
    "y = y.map(lambda x: 1 if x=='RB' else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95953047-56d4-49c0-9389-368b06367624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criação do train + test e validation set\n",
    "X_Train, X_Test, y_Train, y_Test = train_test_split(X, y, test_size=0.25, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda958e-9d88-4946-b8c8-1428a02214f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f7f0e-fa37-4a5e-9679-bbbfa03c0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece845d-bca0-46d5-a758-98a6317ac1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Maximum missing attributes on the rows: {X_Train.isna().sum(axis=1).max()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7033c5d1-4302-421f-a0d0-acf35d114eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = X_Train.isna().sum()\n",
    "missing_cols[missing_cols>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ab5e9-30fa-4fdf-93ed-c35fd21ab2ca",
   "metadata": {},
   "source": [
    "Number of null values is significant on many columns ( > 25% ) <br>\n",
    "Droping features is not an option for dealing with missing data, because we do not have the knowledge yet if they have relation with the class we want to predict<br>\n",
    "\n",
    "However, per sample, 6 out of 40 attributes doesn't seem very significant.\n",
    "This before the feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedb695-402b-4ef7-a6df-bb45192fe246",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification Models\n",
    "\n",
    "- ~[ ] Logit~\n",
    "- ~[ ] LDA~\n",
    "- [x] SVM\n",
    "- ~[ ] Naive Bayes~\n",
    "- ~[ ] DecisionTree~\n",
    "- ~[ ] KNN~\n",
    "- [x] Bagging:\n",
    "    - RandomForest\n",
    "    - KNN\n",
    "- ~[ ] Boosting~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400eb92-925f-4665-9e7a-39ffb4b69dd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Testing Imputation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e44b0-6023-453e-8bd4-c19432d2dbfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test with MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd24b5-9e87-4e80-9294-0f9fe13071e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_not_nan = X_Train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c49ff8-be8d-4ded-8bce-130095fbee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_not_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0959cd7-0543-43af-87e8-0e6e3da41ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3fa52d-c034-4f15-99c9-42ec6c127636",
   "metadata": {},
   "source": [
    "The difference in the number of rows, from the variable *X_train_not_nan* and the variable *X_train* indicates that a huge number of instances are missing at least one of the features, hence droping rows is not a viable option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51957cd9-b808-41ee-8759-fc245fc0fb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = X_Train.isna().sum()/X_Train.shape[0]\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de6a24-dcb5-46bc-84f3-4968cd1bdfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(X,priors):\n",
    "    masks = np.empty(shape = X.shape, dtype=np.bool_)\n",
    "    for i, p in enumerate(priors):\n",
    "        masks[:, i] = np.random.choice((True,False), size=masks.shape[0], p=(p,1-p))\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9743cae7-995a-42ba-9aa4-0d368758acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().fit(X_train_not_nan)\n",
    "X_train_not_nan_scaled = pd.DataFrame(data = scaler.transform(X_train_not_nan),\n",
    "                                      columns=X_train_not_nan.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc1b69-58f0-4241-a3ab-1067ce969cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "masks = [get_mask(X_train_not_nan, priors) for _ in range(N)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206bb6d-d13e-440c-9138-14b30bb30af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = (\n",
    "        SimpleImputer(),\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        KNNImputer()\n",
    ")\n",
    "labels = [\"SimpleImpute_mean\", \"SimpleImpute_median\", \"KNN\"]\n",
    "\n",
    "results = pd.DataFrame(index=X_train_not_nan.columns)\n",
    "for label, model in zip(labels,imputers):\n",
    "    errors=pd.DataFrame(columns = X_train_not_nan.columns)\n",
    "    for _ in range(N):\n",
    "        X_masked = X_train_not_nan_scaled.mask(masks[_])\n",
    "        \n",
    "        model = model.fit(X_masked)\n",
    "        X_imputed = model.transform(X_masked)\n",
    "\n",
    "        errors.loc[_] = dict(zip(X_train_not_nan_scaled.columns, \n",
    "                                 mean_squared_error(X_train_not_nan_scaled, \n",
    "                                                    X_imputed, \n",
    "                                                    squared=False, \n",
    "                                                    multioutput=\"raw_values\")\n",
    "                                ))\n",
    "    results[label] = errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95896c1c-8457-4562-aa71-dba67806ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7f464-4567-4c63-bb44-d9fe6aa47350",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ec36b-db6f-4612-8b90-41729d797d0f",
   "metadata": {},
   "source": [
    "The *KNNImputer* is the one that better predicts the missing values, according to this test, since it is the one that gets closer results for every feature with missing values, which results having the least summed error. <br>\n",
    "Not many different parameters were used for it, so it can probably achieve even better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c796b3-50ae-4d73-a68c-ace6c190eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = (\n",
    "        KNNImputer(n_neighbors=i) for i in range(3,11)\n",
    ")\n",
    "\n",
    "labels = [f\"KNN_{i}_neighbors\" for i in range(3,11)]\n",
    "\n",
    "results = pd.DataFrame(index=X_train_not_nan.columns)\n",
    "for label, model in zip(labels,imputers):\n",
    "    errors=pd.DataFrame(columns = X_train_not_nan.columns)\n",
    "    for _ in range(N):\n",
    "        X_masked = X_train_not_nan_scaled.mask(masks[_])\n",
    "        \n",
    "        model = model.fit(X_masked)\n",
    "        X_imputed = model.transform(X_masked)\n",
    "\n",
    "        errors.loc[_] = dict(zip(X_train_not_nan_scaled.columns, \n",
    "                                 mean_squared_error(X_train_not_nan_scaled, \n",
    "                                                    X_imputed, \n",
    "                                                    squared=False, \n",
    "                                                    multioutput=\"raw_values\")\n",
    "                                ))\n",
    "    results[label] = errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943e36b-026d-44ce-8164-4d85fe487ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa281a8-454b-4cb2-98c4-e91bdf072670",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6b28b-0dc6-4759-8905-9f2635449f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum().idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054a774-6f41-4b07-b631-c528f9821615",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7c4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat( (X_Train, y_Train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490cb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalização por Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "X_Train_scaled=scaler.fit_transform(X_train_not_nan)\n",
    "X_train_stdScaler=pd.DataFrame(\n",
    "    data = X_Train_scaled,\n",
    "    columns=X_Train.columns\n",
    ")\n",
    "X_train_stdScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = (\n",
    "        SimpleImputer(),\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        KNNImputer()\n",
    ")\n",
    "labels = [\"SimpleImpute_mean\", \"SimpleImpute_median\", \"KNN\"]\n",
    "\n",
    "results = pd.DataFrame(index=X_train_not_nan.columns)\n",
    "for label, model in zip(labels,imputers):\n",
    "    errors=pd.DataFrame(columns = X_train_not_nan.columns)\n",
    "    for _ in range(N):\n",
    "        X_masked = X_train_stdScaler.mask(masks[_])\n",
    "        \n",
    "        model = model.fit(X_masked)\n",
    "        X_imputed = model.transform(X_masked)\n",
    "\n",
    "        errors.loc[_] = dict(zip(X_train_stdScaler.columns, \n",
    "                                 mean_squared_error(X_train_stdScaler, \n",
    "                                                    X_imputed, \n",
    "                                                    squared=False, \n",
    "                                                    multioutput=\"raw_values\")\n",
    "                                ))\n",
    "    results[label] = errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a55c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886e14c-978a-492d-a360-fbd0abb86f0d",
   "metadata": {},
   "source": [
    "Similarly to the MinMaxScaler, KNN imputer is the model that has the least error on it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615a5b3-1c90-486d-9c77-e44ad691c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = (\n",
    "        KNNImputer(n_neighbors=i) for i in range(3,11)\n",
    ")\n",
    "\n",
    "labels = [f\"KNN_{i}_neighbors\" for i in range(3,11)]\n",
    "\n",
    "results = pd.DataFrame(index=X_train_not_nan.columns)\n",
    "for label, model in zip(labels,imputers):\n",
    "    errors=pd.DataFrame(columns = X_train_not_nan.columns)\n",
    "    for _ in range(N):\n",
    "        X_masked = X_train_stdScaler.mask(masks[_])\n",
    "        \n",
    "        model = model.fit(X_masked)\n",
    "        X_imputed = model.transform(X_masked)\n",
    "\n",
    "        errors.loc[_] = dict(zip(X_train_stdScaler.columns, \n",
    "                                 mean_squared_error(X_train_stdScaler, \n",
    "                                                    X_imputed, \n",
    "                                                    squared=False, \n",
    "                                                    multioutput=\"raw_values\")\n",
    "                                ))\n",
    "    results[label] = errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90292ea-1cc9-430a-bc8a-c7d0a4794c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9289c6-6412-472e-a4c2-96de5c0b2fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5593b760-5cf2-4a7d-a3aa-6de8f6540b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum().idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791944a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test with Power Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FAZER FUNÇAO PARA NAO REPETIR O CODIGO DEPOIS\n",
    "\n",
    "#Normalização por Power Transform\n",
    "\n",
    "X_Train_powerTscaled=PowerTransformer().fit_transform(X_train_not_nan)\n",
    "X_train_powerTscaler=pd.DataFrame(\n",
    "    data = X_Train_powerTscaled,\n",
    "    columns=X_Train.columns\n",
    ")\n",
    "#X_train_powerTscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f901433",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = (\n",
    "        SimpleImputer(),\n",
    "        SimpleImputer(strategy=\"median\"),\n",
    "        KNNImputer()\n",
    ")\n",
    "labels = [\"SimpleImpute_mean\", \"SimpleImpute_median\", \"KNN\"]\n",
    "\n",
    "results = pd.DataFrame(index=X_train_not_nan.columns)\n",
    "for label, model in zip(labels,imputers):\n",
    "    errors=pd.DataFrame(columns = X_train_not_nan.columns)\n",
    "    for _ in range(N):\n",
    "        X_masked = X_train_powerTscaler.mask(masks[_])\n",
    "        \n",
    "        model = model.fit(X_masked)\n",
    "        X_imputed = model.transform(X_masked)\n",
    "\n",
    "        errors.loc[_] = dict(zip(X_train_powerTscaler.columns, \n",
    "                                 mean_squared_error(X_train_powerTscaler, \n",
    "                                                    X_imputed, \n",
    "                                                    squared=False, \n",
    "                                                    multioutput=\"raw_values\")\n",
    "                                ))\n",
    "    results[label] = errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6702cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21d901-556a-4660-9d70-34d55c4b265c",
   "metadata": {},
   "source": [
    "As we can see, KNN has the minimum error in every scalling method, so we will only use KNN-Imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a2f47-68cd-4bfe-8aec-43c14bbf3be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputers = (\n",
    "        KNNImputer(n_neighbors=i) for i in range(3,11)\n",
    ")\n",
    "\n",
    "labels = [f\"KNN_{i}_neighbors\" for i in range(3,11)]\n",
    "\n",
    "results = pd.DataFrame(index=X_train_not_nan.columns)\n",
    "for label, model in zip(labels,imputers):\n",
    "    errors=pd.DataFrame(columns = X_train_not_nan.columns)\n",
    "    for _ in range(N):\n",
    "        X_masked = X_train_powerTscaler.mask(masks[_])\n",
    "        \n",
    "        model = model.fit(X_masked)\n",
    "        X_imputed = model.transform(X_masked)\n",
    "\n",
    "        errors.loc[_] = dict(zip(X_train_powerTscaler.columns, \n",
    "                                 mean_squared_error(X_train_powerTscaler, \n",
    "                                                    X_imputed, \n",
    "                                                    squared=False, \n",
    "                                                    multioutput=\"raw_values\")\n",
    "                                ))\n",
    "    results[label] = errors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f3b29-09b9-45b7-bdc4-2cb2cf8e9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93b8b4-4860-428d-b805-60945d8e57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results>0].dropna().sum().idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc36a36-7655-4cf8-b132-c587ea885a65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imputer Results\n",
    "\n",
    "For all the cases tested, the 3-NN imputer was the one that obtained better results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b989d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Scale and impute the rest of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c90a3",
   "metadata": {},
   "source": [
    "Perform imputation of missing values before scaling, as scaling could lead to distorted data if the missing values are not first replaced. This is because some calculations may include the missing values and their presence could lead to skewed results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af25d77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Impute data with knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df904d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the KNNImputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "X_Train_imputed = imputer.fit_transform(X_Train)\n",
    "\n",
    "X_Train_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547ad57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scale with PowerTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = PowerTransformer()\n",
    "X_Train_imputed_powerT = pt.fit_transform(X_Train_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d5525b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Selection using RandomForest\n",
    "\n",
    "Fitting a tree find the best features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b732e2-7996-4005-a31a-1a6bff98b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_labels(col_names : list[str], pipeline: Pipeline) -> list[str]:\n",
    "    #join names and scores in tuples (column, score)\n",
    "    label_scores = zip(col_names, pipeline[\"selector\"].estimator_.feature_importances_)\n",
    "    \n",
    "    #sort tuples accoding to value in index 1 (column, -> score <-)\n",
    "    sorted_labels = sorted(label_scores, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d19c9c-77ee-4946-a5ab-ad3dbf1a5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_mat(y_test, pred):\n",
    "    ConfusionMatrixDisplay(\n",
    "        confusion_matrix(y_test, pred)\n",
    "    ).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b679afa-50ae-47ec-9a5b-3a9de83148e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_Train.std()==0).any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d340def7-6f58-491c-a73f-47804c75ec6c",
   "metadata": {},
   "source": [
    "There are no features with constant values, so there is no need to drop any column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0a208-8d25-431f-909a-fe05b2386125",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame(columns=[\"Std dev\", \"Corr with Biodegradable\"])\n",
    "temp_df[\"Corr with Biodegradable\"] = np.abs(pd.concat((X_Train, y_Train), axis=1).corr()[\"Biodegradable\"]).sort_values(ascending=False)\n",
    "temp_df[\"Std dev\"] = X_Train.std()**2\n",
    "\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb128e7e-127f-415a-bd93-f8e57cd7fc2e",
   "metadata": {},
   "source": [
    "> FALAR DE IMPORTÂNCIA DO SCORE DAS FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aae5dce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models with MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90239f-4cf2-4dbe-b47f-1a7450200bbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc295bc-2e9a-4752-a887-36ce325f815d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### First test, testing wit higher jumps to check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33145204-6f5c-4298-a127-5d9c85c2607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"MinMax-RF_v3\"\n",
    "if os.path.exists(FILENAME):\n",
    "    #if file exists, load it\n",
    "    search_results = joblib.load(FILENAME)\n",
    "else:\n",
    "    pipeline = [\n",
    "                (\"scaler\", MinMaxScaler()),\n",
    "                (\"imputer\", KNNImputer(n_neighbors=3)),\n",
    "                (\"selector\", SelectFromModel(\n",
    "                    estimator = RandomForestClassifier(\n",
    "                        random_state=0\n",
    "                    ),\n",
    "                    threshold=-np.inf,\n",
    "                )),\n",
    "                (\"classifier\", RandomForestClassifier(\n",
    "                        min_samples_leaf= 1,\n",
    "                ))\n",
    "               ]\n",
    "\n",
    "    model = Pipeline(pipeline)\n",
    "\n",
    "    grid = {\n",
    "        #{pipeline_name}__{feature_name} : [ ... ]\n",
    "        \"selector__max_features\": range(22, 37+1, 1),\n",
    "        \"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "        \"classifier__max_depth\" : range(25, 35+1, 1),\n",
    "        #classifier__min_samples_leaf : range(1,10)\n",
    "    } \n",
    "\n",
    "    search_results = GridSearchCV(estimator = model,\n",
    "                                  param_grid=grid,\n",
    "                                  cv = 9,\n",
    "                                  scoring=make_scorer(matthews_corrcoef),\n",
    "                                  n_jobs=-1, #number of processes; -1 --> use all\n",
    "                                  verbose=10, #text information\n",
    "                                  return_train_score=True\n",
    "                                 )\n",
    "    search_results.fit(X_Train, y_Train)\n",
    "    #save file\n",
    "    joblib.dump(search_results, FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2697d-f8ff-4a68-84d2-20f1a8771c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8749a89-fe55-415a-a75f-7d831cb0ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters Test Score:\", max(search_results.cv_results_[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e076a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Search results\n",
    "#\n",
    "#\n",
    "method=\"gini\"\n",
    "is_2nd_half = 1 if method==\"gini\" else 0 \n",
    "size = int(len(search_results.cv_results_[\"mean_test_score\"]) / 2)\n",
    "\n",
    "data = list(filter(lambda x: x[\"classifier__criterion\"]==method, search_results.cv_results_[\"params\"]))\n",
    "\n",
    "#getting list of max depth values\n",
    "x_max_depth = list(map(lambda x: x[\"classifier__max_depth\"], data))\n",
    "#min samples leaf list\n",
    "y_min_samples_leaf = list(map(lambda x: x[\"classifier__min_samples_leaf\"], data))\n",
    "#max features list\n",
    "z_max_features = list(map(lambda x: x[\"selector__max_features\"], data))\n",
    "\n",
    "#test scores list\n",
    "#only gets half the data because subselection of \"gini\" data (in cv_results_ we can see that \n",
    "# the first results are all with gini and the rest is using entropy )\n",
    "scores = search_results.cv_results_[\"mean_test_score\"][ is_2nd_half*size : (is_2nd_half+1)*size ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47def61-f36e-4919-b30b-4c25d230aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sorted_labels(X_Train.columns, search_results.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0769be-abe5-4ee6-b6f4-76116d98d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(lambda item: item[1]>0.02, get_sorted_labels(X_Train.columns, search_results.best_estimator_)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05650ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "im = plt.scatter(x_max_depth, z_max_features, c=scores)\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f348a2f-5681-4d86-bf84-5a9e476da0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#plt.figure(figsize=(12,12))\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,8), subplot_kw={\"projection\":\"3d\"})\n",
    "im = ax.scatter(x_max_depth, \n",
    "           y_min_samples_leaf, \n",
    "           z_max_features, \n",
    "           c = scores,\n",
    "           cmap=plt.viridis(),\n",
    "            #s=50\n",
    "          )\n",
    "ax.set_xlabel(\"Max Depth\")\n",
    "ax.set_ylabel(\"Min Samples Leaf\")\n",
    "ax.set_zlabel(\"Max Nº of Features\")\n",
    "\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.ax.set_ylabel('MCC Score', rotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024eee0-18b3-4b87-91b9-d6c3e239541e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N,M=X_Train_imputed.shape\n",
    "\n",
    "rfr=RandomForestRegressor(random_state=0)\n",
    "#threshold is minus infinity\n",
    "sel = SelectFromModel(estimator=rfr, threshold=-np.inf, max_features=5)\n",
    "\n",
    "sel.fit(X_Train_imputed_powerT, y_Train)\n",
    "\n",
    "print(\"Importances: \", sel.estimator_.feature_importances_)\n",
    "\n",
    "print(\"Default threshold: \", sel.threshold_)\n",
    "\n",
    "features=sel.get_support()\n",
    "Features_selected =np.arange(M)[features]\n",
    "print(\"The features selected are columns: \", Features_selected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c3216-702c-4bd5-b9b6-19c4ed599805",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Power Tranformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a4cdb",
   "metadata": {},
   "source": [
    "### Another way to do the same. choose later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14433e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_Train_imputed_powerT, y_Train)\n",
    "\n",
    "\n",
    "# create a dictionary to store your feature importance scores\n",
    "feature_imp = dict(zip(X_Train.columns, rfr.feature_importances_))\n",
    "\n",
    "# specify the number of variables you want\n",
    "num_vars = 5 # insert number of desired variables\n",
    "\n",
    "# create an empty list to store\n",
    "selected_features = []\n",
    "\n",
    "# loop through each variable, sorted by their importance scores\n",
    "for variable, score in sorted(feature_imp.items(), key=lambda x: x[1], reverse=True):\n",
    "  # add the variable if below the specified number of variables\n",
    "  if len(selected_features) < num_vars:\n",
    "    selected_features.append(variable)\n",
    "\n",
    "selected_features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
